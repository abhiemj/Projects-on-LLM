{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bhishekds/tokenization-rnn-lstm-architecture-with-py?scriptVersionId=154282196\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":null,"id":"70e11bdb","metadata":{"papermill":{"duration":0.011561,"end_time":"2023-12-09T15:12:19.334175","exception":false,"start_time":"2023-12-09T15:12:19.322614","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","id":"24afd066","metadata":{"papermill":{"duration":0.008863,"end_time":"2023-12-09T15:12:19.35236","exception":false,"start_time":"2023-12-09T15:12:19.343497","status":"completed"},"tags":[]},"source":["# 1. Load Libraries"]},{"cell_type":"code","execution_count":1,"id":"e5a7b65e","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-09T15:12:19.372661Z","iopub.status.busy":"2023-12-09T15:12:19.372019Z","iopub.status.idle":"2023-12-09T15:12:24.644402Z","shell.execute_reply":"2023-12-09T15:12:24.643688Z"},"papermill":{"duration":5.284936,"end_time":"2023-12-09T15:12:24.64634","exception":false,"start_time":"2023-12-09T15:12:19.361404","status":"completed"},"tags":[]},"outputs":[],"source":["import time\n","import numpy as np\n","import pandas as pd\n","from torchtext import vocab ## for glove vectors\n","import torch\n","from torch import nn\n","from torch.utils.data import TensorDataset, DataLoader\n","# from torchtext.data import TabularDataset\n","from nltk import word_tokenize"]},{"cell_type":"code","execution_count":2,"id":"507dcc95","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:24.66628Z","iopub.status.busy":"2023-12-09T15:12:24.665502Z","iopub.status.idle":"2023-12-09T15:12:24.670505Z","shell.execute_reply":"2023-12-09T15:12:24.669399Z"},"papermill":{"duration":0.016383,"end_time":"2023-12-09T15:12:24.672138","exception":false,"start_time":"2023-12-09T15:12:24.655755","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["2.0.0+cpu\n"]}],"source":["print(torch.__version__)"]},{"cell_type":"code","execution_count":3,"id":"43555026","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:24.691491Z","iopub.status.busy":"2023-12-09T15:12:24.690758Z","iopub.status.idle":"2023-12-09T15:12:26.84331Z","shell.execute_reply":"2023-12-09T15:12:26.842388Z"},"papermill":{"duration":2.164249,"end_time":"2023-12-09T15:12:26.84534","exception":false,"start_time":"2023-12-09T15:12:24.681091","status":"completed"},"tags":[]},"outputs":[],"source":["train = pd.read_csv(r'/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv')\n","# validation = pd.read_csv(r'/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')"]},{"cell_type":"code","execution_count":4,"id":"5032b8d3","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:26.865046Z","iopub.status.busy":"2023-12-09T15:12:26.86478Z","iopub.status.idle":"2023-12-09T15:12:26.869065Z","shell.execute_reply":"2023-12-09T15:12:26.868157Z"},"papermill":{"duration":0.015944,"end_time":"2023-12-09T15:12:26.870746","exception":false,"start_time":"2023-12-09T15:12:26.854802","status":"completed"},"tags":[]},"outputs":[],"source":["# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","## Take small sample \n","train = train[:1000]\n","train2 = train.copy()\n","# validation = validation[:1000]"]},{"cell_type":"code","execution_count":null,"id":"e237077f","metadata":{"papermill":{"duration":0.008651,"end_time":"2023-12-09T15:12:26.888948","exception":false,"start_time":"2023-12-09T15:12:26.880297","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","id":"82385531","metadata":{"papermill":{"duration":0.008634,"end_time":"2023-12-09T15:12:26.906374","exception":false,"start_time":"2023-12-09T15:12:26.89774","status":"completed"},"tags":[]},"source":["# 2. Preprocessing\n","\n","### A. train data"]},{"cell_type":"code","execution_count":5,"id":"57a6caea","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:26.925395Z","iopub.status.busy":"2023-12-09T15:12:26.925103Z","iopub.status.idle":"2023-12-09T15:12:26.936378Z","shell.execute_reply":"2023-12-09T15:12:26.935507Z"},"papermill":{"duration":0.02305,"end_time":"2023-12-09T15:12:26.938319","exception":false,"start_time":"2023-12-09T15:12:26.915269","status":"completed"},"tags":[]},"outputs":[],"source":["## preprocessing \n","import nltk \n","from nltk.corpus import stopwords\n","\n","stopword = stopwords.words('english')\n","\n","# define a function to remove stopwords\n","def remove_stopwords(txt):\n","    filtered_word = []\n","    words = txt.split(' ')\n","    for word in words:\n","        if word not in stopword:\n","            filtered_word.append(word)\n","    return ' '.join(filtered_word)\n","\n","## clean text remove all special characters\n","import re, string\n","\n","def clean_text(txt):\n","    txt = str(txt).lower()\n","    txt = re.sub('\\[.*?\\]','',txt)\n","    txt = re.sub('https?://\\S+|www\\.\\S+','',txt)\n","    txt = re.sub('<.*?>+','',txt)\n","    txt =re.sub('[%s]' % re.escape(string.punctuation),'',txt)\n","    txt = re.sub('\\n','',txt)\n","    txt = re.sub('\\w*\\d\\w*','',txt)\n","    return txt"]},{"cell_type":"code","execution_count":6,"id":"a0b3fff3","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:26.95743Z","iopub.status.busy":"2023-12-09T15:12:26.957129Z","iopub.status.idle":"2023-12-09T15:12:27.128134Z","shell.execute_reply":"2023-12-09T15:12:27.127473Z"},"papermill":{"duration":0.182849,"end_time":"2023-12-09T15:12:27.13009","exception":false,"start_time":"2023-12-09T15:12:26.947241","status":"completed"},"tags":[]},"outputs":[],"source":["train['comment_text'] = train['comment_text'].apply(clean_text)\n","train['comment_text'] = train['comment_text'].apply(remove_stopwords)\n","\n","# train.to_csv('/kaggle/working/' + 'train.csv')"]},{"cell_type":"code","execution_count":7,"id":"bce65465","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:27.15016Z","iopub.status.busy":"2023-12-09T15:12:27.149535Z","iopub.status.idle":"2023-12-09T15:12:27.153308Z","shell.execute_reply":"2023-12-09T15:12:27.152512Z"},"papermill":{"duration":0.015109,"end_time":"2023-12-09T15:12:27.154872","exception":false,"start_time":"2023-12-09T15:12:27.139763","status":"completed"},"tags":[]},"outputs":[],"source":["## Load Data into DataLoader"]},{"cell_type":"markdown","id":"811437f4","metadata":{"papermill":{"duration":0.008663,"end_time":"2023-12-09T15:12:27.172448","exception":false,"start_time":"2023-12-09T15:12:27.163785","status":"completed"},"tags":[]},"source":["## B. Validation Data\n","\n"]},{"cell_type":"code","execution_count":8,"id":"56e36e70","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:27.226377Z","iopub.status.busy":"2023-12-09T15:12:27.226056Z","iopub.status.idle":"2023-12-09T15:12:27.230032Z","shell.execute_reply":"2023-12-09T15:12:27.229188Z"},"papermill":{"duration":0.016201,"end_time":"2023-12-09T15:12:27.231754","exception":false,"start_time":"2023-12-09T15:12:27.215553","status":"completed"},"tags":[]},"outputs":[],"source":["# ## we need to get the stopwords for all five languagee\n","# #append all the other languages stopwords\n","\n","# tuk_stopwords = stopwords.words('turkish')\n","# rus_stopwords = stopwords.words('russian')\n","# ital_stopwords = stopwords.words('italian')\n","# french_stopwords = stopwords.words('french')\n","# portugese_stopwords = stopwords.words('portuguese')\n","# spanish_stopwords = stopwords.words('spanish')\n","\n","# test_stopwords = tuk_stopwords + rus_stopwords + ital_stopwords + french_stopwords + portugese_stopwords + spanish_stopwords\n","\n","# ## define a funct to remove stopwords from test data\n","# def test_stopword(txt):\n","#     words = txt.split(' ')\n","#     filtered_word = []\n","#     for word in words:\n","#         if word not in test_stopwords:\n","#             filtered_word.append(word)\n","#     return ' '.join(filtered_word)\n","\n","# ## remove stopwords from test data \n","# # test['content'] = test['content'].apply(test_stopword)\n","# # test['content'] = test['content'].apply(clean_text)"]},{"cell_type":"code","execution_count":9,"id":"6777046b","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:27.250677Z","iopub.status.busy":"2023-12-09T15:12:27.250348Z","iopub.status.idle":"2023-12-09T15:12:27.254213Z","shell.execute_reply":"2023-12-09T15:12:27.253401Z"},"papermill":{"duration":0.01559,"end_time":"2023-12-09T15:12:27.25625","exception":false,"start_time":"2023-12-09T15:12:27.24066","status":"completed"},"tags":[]},"outputs":[],"source":["# ## validation stopwords\n","# validation_stopword = tuk_stopwords + ital_stopwords + spanish_stopwords\n","\n","# def valid_stop(txt):\n","#     words = txt.split(' ')\n","#     filtered_word = []\n","#     for word in words:\n","#         if word not in validation_stopword:\n","#             filtered_word.append(word)\n","#     return ' '.join(filtered_word)\n","\n","# validation['comment_text'] = validation['comment_text'].apply(valid_stop)\n","# validation['comment_text'] = validation['comment_text'].apply(clean_text)\n","\n","# validation.head()"]},{"cell_type":"code","execution_count":10,"id":"eea7cea6","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:27.275754Z","iopub.status.busy":"2023-12-09T15:12:27.275499Z","iopub.status.idle":"2023-12-09T15:12:27.279646Z","shell.execute_reply":"2023-12-09T15:12:27.278884Z"},"papermill":{"duration":0.015461,"end_time":"2023-12-09T15:12:27.281361","exception":false,"start_time":"2023-12-09T15:12:27.2659","status":"completed"},"tags":[]},"outputs":[],"source":["# #Approach 1 \n","# # tokenize the comment_text,\n","# ## use toxic as label \n","# ## convert to tensor \n","# ## \n","\n","# from nltk import word_tokenize\n","\n","# # tokenize function\n","# def tokenize_text(txt):\n","#     txt = txt.lower()\n","#     tokens = []\n","#     words = txt.split(' ')\n","#     for word in words:\n","#         token = word_tokenize(word)\n","#         tokens.append(token)\n","#     return tokens\n","\n","\n","# ## use train['comment_text'] \n","# ## get all the words \n","# ## get  unique words  create vocab (tokenization is done of unique words)\n","# ## tokenize them \n","# ## use those to create index \n","# ## \n","\n","# ## Builduing vocabulary \n","# # count the words\n","\n","# tokenized_texts = train['comment_text'].apply(tokenize_text)\n","        \n","    \n"]},{"cell_type":"code","execution_count":11,"id":"b2ffcad9","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:27.300263Z","iopub.status.busy":"2023-12-09T15:12:27.299978Z","iopub.status.idle":"2023-12-09T15:12:27.30415Z","shell.execute_reply":"2023-12-09T15:12:27.303375Z"},"papermill":{"duration":0.015722,"end_time":"2023-12-09T15:12:27.305901","exception":false,"start_time":"2023-12-09T15:12:27.290179","status":"completed"},"tags":[]},"outputs":[],"source":["# ## create a library of unique_words\n","# unique_word = []\n","# for i, j in enumerate(train['comment_text']):\n","#     words = j.split(' ')\n","#     for word in words:\n","#         if word not in unique_words:\n","#             unique_word.append(word)"]},{"cell_type":"code","execution_count":12,"id":"c0e66081","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:27.324939Z","iopub.status.busy":"2023-12-09T15:12:27.324601Z","iopub.status.idle":"2023-12-09T15:12:27.329169Z","shell.execute_reply":"2023-12-09T15:12:27.328369Z"},"papermill":{"duration":0.015888,"end_time":"2023-12-09T15:12:27.330703","exception":false,"start_time":"2023-12-09T15:12:27.314815","status":"completed"},"tags":[]},"outputs":[],"source":["# from nltk.tokenize import word_tokenize\n","\n","# #1. tokenize each comment\n","# tokenized_text = []\n","# for i,j in enumerate(train['comment_text']):\n","#         word = j.lower()\n","#         x = word_tokenize(word)\n","#         tokenized_text.append(x)\n","            \n","            \n","# #2. Get unique words\n","# unique_word = set()\n","\n","# for sentence in tokenized_text:\n","#     for word in sentence:\n","#         unique_word.add(word)\n","        \n","                       \n","# # 3. sort alphabetically \n","# unique_word = list[unique_word]\n","# unique_word = sorted(unique_word)\n","\n","# #4. Assign id to each token\n","# word_to_id = {}\n","# for id, word in enumerate(unique_word):\n","#     word_to_id[word] = id\n","\n","# #6. Assign id to word\n","# id_to_word = {}\n","# for token,id in word_to_id.items():\n","#     id_to_word[id] =token\n","    \n","# #7. Add padding to the snetences which has less length\n","\n","\n"]},{"cell_type":"code","execution_count":13,"id":"ada0d5f6","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:27.349746Z","iopub.status.busy":"2023-12-09T15:12:27.349404Z","iopub.status.idle":"2023-12-09T15:12:27.353196Z","shell.execute_reply":"2023-12-09T15:12:27.352402Z"},"papermill":{"duration":0.015368,"end_time":"2023-12-09T15:12:27.354962","exception":false,"start_time":"2023-12-09T15:12:27.339594","status":"completed"},"tags":[]},"outputs":[],"source":["# train['comment_text']"]},{"cell_type":"markdown","id":"a7668b9b","metadata":{"papermill":{"duration":0.008737,"end_time":"2023-12-09T15:12:27.372525","exception":false,"start_time":"2023-12-09T15:12:27.363788","status":"completed"},"tags":[]},"source":["## 3. Convert the text to tokenize for the model "]},{"cell_type":"code","execution_count":14,"id":"69ac6ed4","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-12-09T15:12:27.392003Z","iopub.status.busy":"2023-12-09T15:12:27.391706Z","iopub.status.idle":"2023-12-09T15:12:27.645576Z","shell.execute_reply":"2023-12-09T15:12:27.644303Z"},"papermill":{"duration":0.266039,"end_time":"2023-12-09T15:12:27.647487","exception":false,"start_time":"2023-12-09T15:12:27.381448","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["0      [2852, 2547, 4888, 8805, 3603, 5132, 2957, 701...\n","1      [2023, 5008, 727, 1540, 3925, 7319, 7924, 8277...\n","2      [3709, 4937, 3925, 6712, 8566, 2532, 9024, 353...\n","3      [5298, 1216, 4916, 6705, 8002, 3982, 9262, 730...\n","4                   [7525, 3699, 1331, 6886, 5873, 8286]\n","                             ...                        \n","995    [3711, 9370, 9096, 9164, 3796, 4720, 6141, 978...\n","996    [1577, 4513, 4432, 4433, 4350, 989, 2190, 4467...\n","997    [478, 8842, 7041, 2822, 546, 3878, 5340, 6680,...\n","998    [8566, 4090, 3843, 2782, 9229, 2902, 7062, 584...\n","999    [8410, 6774, 302, 3560, 9068, 1332, 5268, 4954...\n","Name: comment_text, Length: 1000, dtype: object\n"]}],"source":["#define tokenizer\n","def tokenizer(txt):\n","    word = word_tokenize(txt.lower())\n","    return word\n","\n","#tokenize sentences\n","train['comment_text'] = train['comment_text'].apply(tokenizer)\n","\n","# print(train['comment_text'])\n","\n","unique_word = set()\n","for sentence in train['comment_text']:\n","    for word in sentence:\n","        unique_word.add(word)\n","        \n","#convert unique words to list\n","unique_word = list(unique_word)\n","\n","# print(unique_word)\n","#create a dictionary of indexes of tokens\n","word_to_idx = {}\n","for idx,word in enumerate(sorted(unique_word)):\n","    word_to_idx[word] = idx\n","    \n","    \n","#convert word to ids\n","def convert_words_to_ids(tokens):\n","    idx = []\n","    for token in tokens:\n","        token_id = word_to_idx.get(token)\n","        idx.append(token_id)\n","    return idx\n","        \n","    \n","\n","# assign ids to words\n","train['comment_text'] = train['comment_text'].apply(convert_words_to_ids)\n","\n","print(train['comment_text'])\n","#max_length of sentence\n","length_of_sequences = []\n","for i,j in enumerate(train['comment_text']):\n","    x = len(train['comment_text'][i])\n","    length_of_sequences.append(x)\n","max_length = max(length_of_sequences)\n","\n","#pad the sentences\n","padded_sentence = []\n","for i ,sentence in enumerate(train['comment_text']):\n","    length = len(sentence)\n","    pad = sentence + [0]*(max_length-length)\n","    padded_sentence.append(pad)"]},{"cell_type":"code","execution_count":15,"id":"f3da18c9","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:27.66691Z","iopub.status.busy":"2023-12-09T15:12:27.666652Z","iopub.status.idle":"2023-12-09T15:12:27.672935Z","shell.execute_reply":"2023-12-09T15:12:27.671982Z"},"papermill":{"duration":0.017843,"end_time":"2023-12-09T15:12:27.674704","exception":false,"start_time":"2023-12-09T15:12:27.656861","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["793"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["max_length"]},{"cell_type":"code","execution_count":16,"id":"b0152daf","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-12-09T15:12:27.694148Z","iopub.status.busy":"2023-12-09T15:12:27.693765Z","iopub.status.idle":"2023-12-09T15:12:27.708393Z","shell.execute_reply":"2023-12-09T15:12:27.707604Z"},"papermill":{"duration":0.026345,"end_time":"2023-12-09T15:12:27.710158","exception":false,"start_time":"2023-12-09T15:12:27.683813","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["[2852,\n"," 2547,\n"," 4888,\n"," 8805,\n"," 3603,\n"," 5132,\n"," 2957,\n"," 7017,\n"," 9102,\n"," 8880,\n"," 1498,\n"," 3322,\n"," 8991,\n"," 5485,\n"," 9424,\n"," 2415,\n"," 2900,\n"," 6173,\n"," 2427,\n"," 6896,\n"," 8227,\n"," 8136,\n"," 5873,\n"," 7513,\n"," 3925,\n"," 7000,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["padded_sentence[0]"]},{"cell_type":"code","execution_count":17,"id":"08719111","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:27.732037Z","iopub.status.busy":"2023-12-09T15:12:27.73142Z","iopub.status.idle":"2023-12-09T15:12:27.736009Z","shell.execute_reply":"2023-12-09T15:12:27.735259Z"},"papermill":{"duration":0.016592,"end_time":"2023-12-09T15:12:27.737424","exception":false,"start_time":"2023-12-09T15:12:27.720832","status":"completed"},"tags":[]},"outputs":[],"source":["# Assign the sentences to \n","train['comment_text'] = padded_sentence"]},{"cell_type":"code","execution_count":18,"id":"5300f100","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:27.757788Z","iopub.status.busy":"2023-12-09T15:12:27.757505Z","iopub.status.idle":"2023-12-09T15:12:27.767706Z","shell.execute_reply":"2023-12-09T15:12:27.766919Z"},"papermill":{"duration":0.022792,"end_time":"2023-12-09T15:12:27.769737","exception":false,"start_time":"2023-12-09T15:12:27.746945","status":"completed"},"tags":[]},"outputs":[],"source":["## CHECK if the words are not padded\n","for i,j in enumerate(train['comment_text']):\n","    if len(train['comment_text'][i]) != max_length:\n","        print(i)"]},{"cell_type":"markdown","id":"da138d71","metadata":{"papermill":{"duration":0.009406,"end_time":"2023-12-09T15:12:27.789552","exception":false,"start_time":"2023-12-09T15:12:27.780146","status":"completed"},"tags":[]},"source":["## 3. DEFINE RNN ARCHITECTURE\n","\n","RNN ARCHITECTURE : INPUT AND OUTPUT\n","\n","https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-rnn-cb6ebc594677#:~:text=torch.-,nn.,batch%2C%20num_directions%20*%20hidden_size)%20.\n","\n","rnn class\n","\n","1. **for unbatched** \n","input tensor (sequence_length,input_size)\n","output tensor(sequence_length,D * hidden_size)\n","\n","D = 2 IF bidirectional =True, otherwise 1\n","\n","\n","2. **when batch_first = False**\n","input tensor(sequence_length,batch_size, input_size), \n","output tensor(sequence_length,batch_size, D*hidden_size)\n","\n","when batch_first = True\n","input tensor(batch_size,sequence_length,input_size)\n","output tensor(batch_size,sequence_length,D*hidden_size)\n","\n","\n","3. **Output of rnn is output and h_n**\n","\n","'output' contains hidden states for each time step of each sequence in the batch\n","Shape of the output varies according to batch_first parameter\n","\n","'h_n' contains the final hidden state for each elements in the batch \n","\n","for unbatched input\n","hn is (D*num_layers,hidden_size)\n","h"]},{"cell_type":"code","execution_count":19,"id":"3193b81c","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:27.810489Z","iopub.status.busy":"2023-12-09T15:12:27.809915Z","iopub.status.idle":"2023-12-09T15:12:27.813964Z","shell.execute_reply":"2023-12-09T15:12:27.813388Z"},"papermill":{"duration":0.016493,"end_time":"2023-12-09T15:12:27.815624","exception":false,"start_time":"2023-12-09T15:12:27.799131","status":"completed"},"tags":[]},"outputs":[],"source":["# ## design model\n","# class RNNmodel(nn.Module): \n","#     def __init__(self,vocab_size,embedding_dimension,hidden_dim,output_dim):\n","#         super(RNNmodel,self).__init__()\n","#         self.embeddinglayer = nn.Embedding(num_embeddings = vocab_size,embedding_dim = embedding_dimension)\n","#         '''num_embeddings -> size of dictionary of embeddings(total number of unique words or tokens\n","#         in your vocabulary).\n","#         embedding_dim -> size of each embedding vector(dimensionality of embeddings)'''\n","#         self.rnn = nn.RNN(input_size=embedding_dimension,hidden_size=hidden_dim)\n","#         self.linear = nn.Linear(hidden_dim,output_dim)\n","#         self.sigmoid = nn.Sigmoid()\n","        \n","#     def forward(self,text):\n","#         embedded_layer = self.embeddinglayer(text)\n","#         out,hidden  = self.rnn(embedded_layer)\n","#         output2 = self.linear(hidden)\n","#             # for classification task we use the output value of all time steps in last rnn\n","#             ## for other seq 2 seq task we use hidden value of last time step \n","#         final = self.sigmoid(output2)\n","#         return final "]},{"cell_type":"markdown","id":"2a0e1c36","metadata":{"papermill":{"duration":0.0097,"end_time":"2023-12-09T15:12:27.835464","exception":false,"start_time":"2023-12-09T15:12:27.825764","status":"completed"},"tags":[]},"source":["The output of the `self.rnn(embedded)` statement in PyTorch, when using a basic RNN layer, consists of two main components: **`output`** and **`hidden`**. Let's break down what each of these represents and their shapes:\n","\n","### `output`\n","- **`output` contains the hidden states for each time step of each sequence in the batch.**\n","- Its shape varies depending on whether the input was batched and the `batch_first` parameter.\n","\n","#### Shape of `output`\n","1. **Unbatched Input (`batch_first=False`)**\n","   - Shape: `(seq_len, batch_size, num_directions * hidden_size)`\n","   - Here, `seq_len` is the length of the sequence, `batch_size` is the number of sequences (if batched), `num_directions` is 1 for a simple RNN (2 for bidirectional RNNs), and `hidden_size` is the size of the hidden layer.\n","\n","2. **Batched Input (`batch_first=True`)**\n","   - Shape: `(batch_size, seq_len, num_directions * hidden_size)`\n","   - The same dimensions as above, but the batch size and sequence length dimensions are swapped.\n","\n","### `hidden`\n","- **`hidden` represents the final hidden state for each element in the batch.**\n","- It's particularly important for understanding the final state of the sequence, which is often used in sequence-to-label tasks (like classification).\n","\n","#### Shape of `hidden`\n","- Shape: `(num_layers * num_directions, batch_size, hidden_size)`\n","- `num_layers` is the number of layers in the RNN, `num_directions` is 1 for a simple RNN (and 2 for bidirectional), `batch_size` is the number of sequences, and `hidden_size` is the size of the hidden layer.\n","\n","### Explanation\n","1. **For Each Time Step (`output`):**\n","   - The RNN processes each element of the sequence one by one. At each time step, it updates its hidden state based on the current input and the previous hidden state.\n","   - The **`output` tensor collects these hidden states for each time step across all sequences in the batch. It's useful if you need to do further processing at each time step (e.g., in sequence-to-sequence models).**\n","\n","2. **Final State of the Sequence (`hidden`):**\n","   - After processing the last element of the sequence, the RNN's hidden state represents the final encoding of the entire sequence.\n","   - The **`hidden` tensor captures this final state. It's especially useful for tasks like classification where the entire sequence's context needs to be considered for a single prediction.**\n","\n","### In Summary\n","- `output`: Provides a full sequence of hidden states, useful for tasks where every time step's output is important.\n","- `hidden`: Provides the final hidden state, useful for tasks where the final state of the sequence is critical (like classification).\n","\n","The structure and shape of these outputs allow RNNs to be versatile for a variety of sequence processing tasks in PyTorch."]},{"cell_type":"markdown","id":"b25cd233","metadata":{"papermill":{"duration":0.009344,"end_time":"2023-12-09T15:12:27.854297","exception":false,"start_time":"2023-12-09T15:12:27.844953","status":"completed"},"tags":[]},"source":["## I was making mistake by not squeezing the hidden dimsion and my output from forward function is coming of size (16,793)\n","\n","The issue you're encountering seems to be related to the dimensions of the output from your RNN model. When using an RNN for binary classification, the typical approach is to use the last hidden state to make a prediction. However, the shape of the output seems to be inconsistent with what is expected for binary classification.\n","\n","Let's break down the potential issues and solutions:\n","\n","1. **Understanding RNN Output:** The `self.rnn` layer in your code returns two outputs: `out` and `hidden`. The `out` contains the hidden states from all timesteps, while `hidden` is the last hidden state. For binary classification, you generally use the last hidden state.\n","\n","2. **Shape of RNN Output:** The output shape of `hidden` from an RNN in PyTorch is `(num_layers * num_directions, batch, hidden_size)`. If you are using a single layer, unidirectional RNN, this shape becomes `(1, batch_size, hidden_size)`. \n","\n","3. **Linear Layer Dimension Issue:** You have defined your linear layer with an output dimension of 2 (`self.linear = nn.Linear(hidden_dim, 2)`). This is suitable for a multi-class classification problem with 2 classes, but for binary classification, you typically have a single output unit which predicts the probability of one class (with the other class's probability being `1 - predicted_probability`). Therefore, your linear layer should have an output dimension of 1 for binary classification.\n","\n","4. **Squeezing the Hidden State:** Before passing the hidden state to the linear layer, you should squeeze the first dimension (which is 1) to match the input shape expectation of the linear layer. \n","\n","5. **Sigmoid Activation:** The sigmoid activation is correctly used for binary classification. However, you should apply it after the linear layer on its output, ensuring that the output is a single value representing the probability of the positive class for each instance in the batch.\n","\n","Here's how you can modify your `forward` method:\n","\n","```python\n","def forward(self, text):\n","    embedded_layer = self.embeddinglayer(text)\n","    out, hidden = self.rnn(embedded_layer)\n","    # hidden is of shape (1, batch_size, hidden_size), squeeze the first dimension\n","    hidden = hidden.squeeze(0) # Now hidden is of shape (batch_size, hidden_size)\n","    output2 = self.linear(hidden) # Now output2 is of shape (batch_size, 1)\n","    final = self.sigmoid(output2)\n","    return final.squeeze()  # Ensuring the output is of shape (batch_size)\n","```\n","\n","And make sure your linear layer is defined for binary classification:\n","\n","```python\n","self.linear = nn.Linear(hidden_dim, 1)\n","```\n","\n","This should correct the dimensionality issue and produce an output tensor that matches the size of your batch, thus resolving the mismatch error."]},{"cell_type":"code","execution_count":20,"id":"f887d383","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:27.874867Z","iopub.status.busy":"2023-12-09T15:12:27.874398Z","iopub.status.idle":"2023-12-09T15:12:27.877807Z","shell.execute_reply":"2023-12-09T15:12:27.877033Z"},"papermill":{"duration":0.015935,"end_time":"2023-12-09T15:12:27.879754","exception":false,"start_time":"2023-12-09T15:12:27.863819","status":"completed"},"tags":[]},"outputs":[],"source":["# vocab_size"]},{"cell_type":"code","execution_count":21,"id":"6e0149a8","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:27.899923Z","iopub.status.busy":"2023-12-09T15:12:27.899635Z","iopub.status.idle":"2023-12-09T15:12:27.906094Z","shell.execute_reply":"2023-12-09T15:12:27.905533Z"},"papermill":{"duration":0.018277,"end_time":"2023-12-09T15:12:27.907518","exception":false,"start_time":"2023-12-09T15:12:27.889241","status":"completed"},"tags":[]},"outputs":[],"source":["## design model\n","class RNNmodel(nn.Module): \n","    def __init__(self,vocab_size,embedding_dimension,hidden_dim,output_dim):\n","        super(RNNmodel,self).__init__()\n","        self.embeddinglayer = nn.Embedding(num_embeddings = vocab_size,embedding_dim = embedding_dimension)\n","        '''num_embeddings -> size of dictionary of embeddings(total number of unique words or tokens\n","        in your vocabulary).\n","        embedding_dim -> size of each embedding vector(dimensionality of embeddings)'''\n","        self.rnn = nn.RNN(input_size=embedding_dimension,hidden_size=hidden_dim,batch_first =True)\n","        self.linear = nn.Linear(hidden_dim,output_dim)\n","        self.sigmoid = nn.Sigmoid()\n","        \n","    def forward(self,text):\n","        embedded_layer = self.embeddinglayer(text)\n","        print(f\"shape of embedded_layer: {embedded_layer.shape}\")\n","        out,hidden  = self.rnn(embedded_layer)\n","        print(f\"Shape of hidden state after rnn ,before squeeze: {hidden.shape}\")\n","        # hidden is of shape (1,batch_size,hidden_size), will squeeze the first dimesnion\n","        hidden = hidden.squeeze(0) # now hidden is of shape (batch_size,hidden_size)\n","        print(f\"shape of input to linear{hidden.shape}\")\n","        output2 = self.linear(hidden)# output 2 is of shape (batch_size,1)\n","        # for seq 2 seq task we use the output value of all time steps in last rnn\n","        ## for classification task we use hidden value of last time step \n","        print(f\"shape of output after linear:{output2.shape}\")\n","        final = self.sigmoid(output2)\n","        return final.squeeze()"]},{"cell_type":"markdown","id":"fc2600cf","metadata":{"papermill":{"duration":0.009507,"end_time":"2023-12-09T15:12:27.926682","exception":false,"start_time":"2023-12-09T15:12:27.917175","status":"completed"},"tags":[]},"source":["**Mistake : I was not putting batch_first = true in nn.rnn\n","(\n","self.rnn = nn.RNN(input_size=embedding_dimension,hidden_size=hidden_dim,batch_first =True)\n",") \n","which caused a error that hidden dimension to be of shape(1,763,256) (batch,sequence_length,hidden_dimesnion), but the hidden state shpuld have a shape of (1,16,256) i.e (1,batch_size,hidden_size) after the rnn**\n","\n","From your provided code and the issue description, it seems that the problem might not be in the model architecture itself, but rather in how the data is being processed or fed into the model. The output size of `793` is particularly intriguing as it does not directly correspond to a typical dimension in an RNN for binary classification (like batch size or hidden size).\n","\n","Here are some potential issues to check:\n","\n","1. **Data Loader and Batch Processing:**\n","   - Ensure that `X_train` and `y_train` in your training loop are tensors of the correct shape. `X_train` should be a tensor of shape `[batch_size, seq_len]`, where `seq_len` is the length of your sequences, and `y_train` should be of shape `[batch_size]`.\n","   - In your training loop, you are iterating over `train_loader` but only using the last batch for training and validation. The line `X_train, y_train = batch` and similar for validation should be inside the loop body, not outside.\n","\n","2. **Input to the Model:**\n","   - Verify that the input text data (`X_train` and `X_val`) is correctly preprocessed and converted to tensor format suitable for input to your RNN. This includes tokenization, numericalization (converting words to indices), and padding if necessary.\n","\n","3. **Dimension of Hidden State in RNN:**\n","   - Double-check that the `hidden_dim` parameter in your RNNmodel class matches the expected size. The hidden state should have a shape of `[1, batch_size, hidden_size]` after the RNN and before squeezing.\n","\n","4. **Output Dimension of Linear Layer:**\n","   - For binary classification, the `output_dim` in your model's constructor should be `1`, and it looks like it might be correctly set. Just verify it.\n","\n","5. **Check Model Output:**\n","   - After getting `y_pred` from your model, print its shape to confirm it is `[batch_size]`. If it's not, there might be an issue in the forward pass of your model.\n","\n","6. **Error in Data Preparation or Model Call:**\n","   - There might be an issue in how the data is being prepared or an inconsistency in how the model is called with the data. Double-check the preprocessing steps.\n","\n","\n"]},{"cell_type":"code","execution_count":22,"id":"c4a9675c","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:27.947071Z","iopub.status.busy":"2023-12-09T15:12:27.946709Z","iopub.status.idle":"2023-12-09T15:12:27.969218Z","shell.execute_reply":"2023-12-09T15:12:27.968248Z"},"papermill":{"duration":0.034542,"end_time":"2023-12-09T15:12:27.970762","exception":false,"start_time":"2023-12-09T15:12:27.93622","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000997932d777bf</td>\n","      <td>[2852, 2547, 4888, 8805, 3603, 5132, 2957, 701...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000103f0d9cfb60f</td>\n","      <td>[2023, 5008, 727, 1540, 3925, 7319, 7924, 8277...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000113f07ec002fd</td>\n","      <td>[3709, 4937, 3925, 6712, 8566, 2532, 9024, 353...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0001b41b1c6bb37e</td>\n","      <td>[5298, 1216, 4916, 6705, 8002, 3982, 9262, 730...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0001d958c54c6e35</td>\n","      <td>[7525, 3699, 1331, 6886, 5873, 8286, 0, 0, 0, ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>02b8e9f1f138d728</td>\n","      <td>[3711, 9370, 9096, 9164, 3796, 4720, 6141, 978...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>02b90e56ec25a4c1</td>\n","      <td>[1577, 4513, 4432, 4433, 4350, 989, 2190, 4467...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>02b91acc085c26f8</td>\n","      <td>[478, 8842, 7041, 2822, 546, 3878, 5340, 6680,...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>02b94ce316048bc1</td>\n","      <td>[8566, 4090, 3843, 2782, 9229, 2902, 7062, 584...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>02b9ef57925866c8</td>\n","      <td>[8410, 6774, 302, 3560, 9068, 1332, 5268, 4954...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows × 8 columns</p>\n","</div>"],"text/plain":["                   id                                       comment_text  \\\n","0    0000997932d777bf  [2852, 2547, 4888, 8805, 3603, 5132, 2957, 701...   \n","1    000103f0d9cfb60f  [2023, 5008, 727, 1540, 3925, 7319, 7924, 8277...   \n","2    000113f07ec002fd  [3709, 4937, 3925, 6712, 8566, 2532, 9024, 353...   \n","3    0001b41b1c6bb37e  [5298, 1216, 4916, 6705, 8002, 3982, 9262, 730...   \n","4    0001d958c54c6e35  [7525, 3699, 1331, 6886, 5873, 8286, 0, 0, 0, ...   \n","..                ...                                                ...   \n","995  02b8e9f1f138d728  [3711, 9370, 9096, 9164, 3796, 4720, 6141, 978...   \n","996  02b90e56ec25a4c1  [1577, 4513, 4432, 4433, 4350, 989, 2190, 4467...   \n","997  02b91acc085c26f8  [478, 8842, 7041, 2822, 546, 3878, 5340, 6680,...   \n","998  02b94ce316048bc1  [8566, 4090, 3843, 2782, 9229, 2902, 7062, 584...   \n","999  02b9ef57925866c8  [8410, 6774, 302, 3560, 9068, 1332, 5268, 4954...   \n","\n","     toxic  severe_toxic  obscene  threat  insult  identity_hate  \n","0        0             0        0       0       0              0  \n","1        0             0        0       0       0              0  \n","2        0             0        0       0       0              0  \n","3        0             0        0       0       0              0  \n","4        0             0        0       0       0              0  \n","..     ...           ...      ...     ...     ...            ...  \n","995      0             0        0       0       0              0  \n","996      0             0        0       0       0              0  \n","997      0             0        0       0       0              0  \n","998      0             0        0       0       0              0  \n","999      0             0        0       0       0              0  \n","\n","[1000 rows x 8 columns]"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["train"]},{"cell_type":"code","execution_count":23,"id":"ce5bb7f4","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:27.992397Z","iopub.status.busy":"2023-12-09T15:12:27.991549Z","iopub.status.idle":"2023-12-09T15:12:28.085617Z","shell.execute_reply":"2023-12-09T15:12:28.084947Z"},"papermill":{"duration":0.106779,"end_time":"2023-12-09T15:12:28.087546","exception":false,"start_time":"2023-12-09T15:12:27.980767","status":"completed"},"tags":[]},"outputs":[],"source":["## load into dataloaders\n","X = train['comment_text']\n","y = train['toxic'].values\n","X_tensor = torch.tensor(X,dtype = torch.long)\n","y_tensor= torch.tensor(y,dtype =torch.float)\n","\n","dataset = TensorDataset(X_tensor,y_tensor)\n","# train_dataset = DataLoader(dataset,batch_size=16,shuffle=True)\n","\n"]},{"cell_type":"code","execution_count":24,"id":"159e7de5","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:28.109604Z","iopub.status.busy":"2023-12-09T15:12:28.108732Z","iopub.status.idle":"2023-12-09T15:12:28.125007Z","shell.execute_reply":"2023-12-09T15:12:28.123848Z"},"papermill":{"duration":0.029482,"end_time":"2023-12-09T15:12:28.127412","exception":false,"start_time":"2023-12-09T15:12:28.09793","status":"completed"},"tags":[]},"outputs":[],"source":["## USing the train data only as validation data because for validation data the unique words needs tonbe added into vocabulary \n","\n","from torch.utils.data import random_split\n","# Split the dataset into training and testing sets\n","train_size = int(0.8 * len(dataset))\n","test_size = len(dataset) - train_size\n","train_dataset, val_dataset = random_split(dataset, [train_size, test_size])\n","\n","# Create data loaders for the train and test sets\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"]},{"cell_type":"code","execution_count":25,"id":"207df92d","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:28.14832Z","iopub.status.busy":"2023-12-09T15:12:28.148038Z","iopub.status.idle":"2023-12-09T15:12:28.194547Z","shell.execute_reply":"2023-12-09T15:12:28.193447Z"},"papermill":{"duration":0.058926,"end_time":"2023-12-09T15:12:28.196358","exception":false,"start_time":"2023-12-09T15:12:28.137432","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([6401,  577,  701, 5148, 5654,  290, 5174, 8947, 9404,  386, 2020,  801,\n","        5174, 1178, 9328, 9321, 9356,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0]) tensor(0.)\n"]}],"source":["for i,j in train_dataset:\n","    print(i,j)\n","    break\n","## i -> X_tensor"]},{"cell_type":"markdown","id":"79094287","metadata":{"papermill":{"duration":0.009734,"end_time":"2023-12-09T15:12:28.216505","exception":false,"start_time":"2023-12-09T15:12:28.206771","status":"completed"},"tags":[]},"source":["i -> X_tensor contains tokenized  text data, i is a subset of this tensor corresponding to the batch size.\n","\n","j -> batch of labels from y_tensor"]},{"cell_type":"code","execution_count":26,"id":"4407de6b","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:28.238128Z","iopub.status.busy":"2023-12-09T15:12:28.237443Z","iopub.status.idle":"2023-12-09T15:12:28.252973Z","shell.execute_reply":"2023-12-09T15:12:28.252335Z"},"papermill":{"duration":0.028184,"end_time":"2023-12-09T15:12:28.254572","exception":false,"start_time":"2023-12-09T15:12:28.226388","status":"completed"},"tags":[]},"outputs":[],"source":["vocab_size = len(unique_word)\n","embedding_dimension = 100\n","hidden_dim = 256\n","output_dim = 1\n","\n","model = RNNmodel(vocab_size = vocab_size,embedding_dimension=embedding_dimension,hidden_dim=hidden_dim,output_dim=output_dim)\n","# model.to(device)\n","\n","\n","## define a levaluation function\n","loss_fn = nn.BCELoss()\n","## optimizer\n","optimizer = torch.optim.Adam(model.parameters(),\n","                            lr = 0.001)\n","# define evaluation \n","from sklearn.metrics import roc_auc_score\n","def roc_score(y_train,y_pred):\n","    y_train = y_train.detach().cpu().numpy()\n","    y_pred = y_pred.detach().cpu().numpy()\n","    roc = roc_auc_score(y_train,y_pred)\n","    return roc"]},{"cell_type":"code","execution_count":27,"id":"e33187f8","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:28.276476Z","iopub.status.busy":"2023-12-09T15:12:28.275944Z","iopub.status.idle":"2023-12-09T15:12:36.56388Z","shell.execute_reply":"2023-12-09T15:12:36.562823Z"},"papermill":{"duration":8.300997,"end_time":"2023-12-09T15:12:36.566211","exception":false,"start_time":"2023-12-09T15:12:28.265214","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([16, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 16, 256])\n","shape of input to lineartorch.Size([16, 256])\n","shape of output after linear:torch.Size([16, 1])\n","shape of embedded_layer: torch.Size([8, 793, 100])\n","Shape of hidden state after rnn ,before squeeze: torch.Size([1, 8, 256])\n","shape of input to lineartorch.Size([8, 256])\n","shape of output after linear:torch.Size([8, 1])\n","epoch 0 | train_loss 0.3851653039455414 | validation loss 0.08693034201860428 \n","CPU times: user 16.1 s, sys: 268 ms, total: 16.4 s\n","Wall time: 8.28 s\n"]}],"source":["%%time\n","## Training\n","\n","epochs = 1\n","\n","\n","\n","for epoch in range(epochs):\n","    model.train()\n","    for batch in train_loader:\n","        X_train , y_train = batch\n","        y_pred = model(X_train)\n","        y_pred = y_pred.squeeze()\n","        loss = loss_fn(y_pred,y_train)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    \n","    model.eval()\n","    with torch.no_grad():\n","        for batches in val_loader:\n","            X_val , y_val = batches\n","            pred = model(X_val)\n","            val_loss = loss_fn(pred,y_val)\n","            #val_roc = roc_score(y_val,pred)\n","    \n","    if epoch%10==0:\n","        print(f\"epoch {epoch} | train_loss {loss} | validation loss {val_loss} \")\n","        \n","        "]},{"cell_type":"code","execution_count":28,"id":"ce3e2bd0","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:36.592999Z","iopub.status.busy":"2023-12-09T15:12:36.592244Z","iopub.status.idle":"2023-12-09T15:12:36.597514Z","shell.execute_reply":"2023-12-09T15:12:36.596842Z"},"papermill":{"duration":0.020407,"end_time":"2023-12-09T15:12:36.599406","exception":false,"start_time":"2023-12-09T15:12:36.578999","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(torch.Size([16]), torch.Size([16]))"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["y_pred.shape,y_train.shape"]},{"cell_type":"code","execution_count":29,"id":"b253b54d","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:36.62544Z","iopub.status.busy":"2023-12-09T15:12:36.625173Z","iopub.status.idle":"2023-12-09T15:12:36.630482Z","shell.execute_reply":"2023-12-09T15:12:36.6298Z"},"papermill":{"duration":0.019957,"end_time":"2023-12-09T15:12:36.632041","exception":false,"start_time":"2023-12-09T15:12:36.612084","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["torch.Size([16, 793])"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["X_train.shape"]},{"cell_type":"markdown","id":"737e6b56","metadata":{"papermill":{"duration":0.012011,"end_time":"2023-12-09T15:12:36.656374","exception":false,"start_time":"2023-12-09T15:12:36.644363","status":"completed"},"tags":[]},"source":["## 5. Architecture of LSTM\n","\n","- 1. Output of **`nn.lstm`**\n","\n","- A. **output** - Contains the output feature(h_t) from last layer of the LSTM, for each t.\n","        -- Used for **`sequence labelling`**(where we want an output for each input in the sequence)\n","\n","\n","- B. **hidden_state(h_n)/short term memory** - Contains the final hidden state for each element in the sequence.\n","        -- Used for **`sentiment analysis`** where we a single output for the entire sequence.\n","\n","\n","- C. **cell_state(c_n)/Long term memory** - Contains the final cell state for each element in the state"]},{"cell_type":"code","execution_count":30,"id":"3a7b66ee","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:36.682774Z","iopub.status.busy":"2023-12-09T15:12:36.682149Z","iopub.status.idle":"2023-12-09T15:12:36.688627Z","shell.execute_reply":"2023-12-09T15:12:36.688075Z"},"papermill":{"duration":0.021423,"end_time":"2023-12-09T15:12:36.690098","exception":false,"start_time":"2023-12-09T15:12:36.668675","status":"completed"},"tags":[]},"outputs":[],"source":["class lstmmodel(nn.Module):\n","    def __init__(self,vocab_size,\n","                     embedding_dimension,\n","                    hidden_dimension,\n","                output_dimesnion):\n","        super(lstmmodel).__init__(self)\n","        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n","                                       embedding_dim = embedding_dimension)\n","#         self.rnn = self.RNN(input_size=embedding_dimension,#input features\n","#                            hidden_size= hidden_dimension,#number of neurons in hidden state h\n","#                            num_layers = 1,## number of stacked rnn\n","#                            bias = True,\n","#                            batch_first = True)\n","        self.lstm = nn.LSTM(input_size = embedding_dimension,#input features\n","                             hidden_size = hidden_dimension,# neurons in hidden layer\n","                             num_layers = 1,#number of lstm layers\n","                             bias= True,\n","                             batch_first = True,\n","                             bidirectional = False# not bidirectional \n","                             )\n","        self.linear = nn.Linear(in_features=hidden_dimension,\n","                                  output_features = output_dimension,\n","                                 bias = True)\n","        self.sigmoid = nn.Sigmoid()\n","        \n","    def forward(self,text):\n","        embedding = self.embedding(text)\n","        out,(hidden_state,cell_state) = self.lstm(embedding) # output of lstm is output,(hidden,)\n","        #squeeze hidden\n","        hidden_state = hidden_state.squeeze(0)\n","        output2 = self.linear(hidden_state)\n","        final = self.sigmoid(output2)\n","        return final\n","        "]},{"cell_type":"markdown","id":"81aa6634","metadata":{"papermill":{"duration":0.01223,"end_time":"2023-12-09T15:12:36.71468","exception":false,"start_time":"2023-12-09T15:12:36.70245","status":"completed"},"tags":[]},"source":["## Tokenize using Count Vectorizer"]},{"cell_type":"code","execution_count":31,"id":"4ea60138","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:36.741103Z","iopub.status.busy":"2023-12-09T15:12:36.740598Z","iopub.status.idle":"2023-12-09T15:12:36.861766Z","shell.execute_reply":"2023-12-09T15:12:36.860861Z"},"papermill":{"duration":0.136908,"end_time":"2023-12-09T15:12:36.863921","exception":false,"start_time":"2023-12-09T15:12:36.727013","status":"completed"},"tags":[]},"outputs":[],"source":["## create Words into vectors using Count Vectorizer\n","train = train2\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","cvec = CountVectorizer()\n","## tokenize using count vectorizer\n","X = cvec.fit_transform(train['comment_text'])\n","y = train['toxic'].values\n","\n","# convert to pytorch tensors\n","from torch.utils.data import TensorDataset, DataLoader\n","X_tensor = torch.tensor(X.toarray()).float()\n","y_tensor = torch.tensor(y).long()\n","\n","## create Tensor Dataset\n","dataset = TensorDataset(X_tensor,y_tensor)\n","\n","train_loader = DataLoader(dataset,batch_size=32,shuffle = True)"]},{"cell_type":"markdown","id":"1caafd8b","metadata":{"papermill":{"duration":0.012237,"end_time":"2023-12-09T15:12:36.889253","exception":false,"start_time":"2023-12-09T15:12:36.877016","status":"completed"},"tags":[]},"source":["**Same way tfidf , glove can be used** \n","\n","- from sklearn.feature selection import tfidf\n","- use it to tokenize the words.\n","- convert the feature and labels to tensors\n","- load it into tensor dataset\n","- load it into dataloaders\n","- design an architecture rnn, lstm \n","- tokens need to be fed into embeddings\n","- first design embedding layer\n","- feed those embeddings to architecture (RNN,LSTM)\n","- **The correct output needs to be used depending on the type of task performed**\n","- Feed those output as per specific task into further nn.modules\n","- Above classification task the output is fed into linear and then sigmoid to convert it into between o to 1 for binary classification.\n","- write the training loop \n","- call the features and label tensors for feeding into model using for loop on dataloaders.\n","- train the loop and call evaluation function to evaluate the model."]},{"cell_type":"code","execution_count":32,"id":"d80660f4","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:36.915418Z","iopub.status.busy":"2023-12-09T15:12:36.915122Z","iopub.status.idle":"2023-12-09T15:12:36.918958Z","shell.execute_reply":"2023-12-09T15:12:36.918028Z"},"papermill":{"duration":0.018817,"end_time":"2023-12-09T15:12:36.92058","exception":false,"start_time":"2023-12-09T15:12:36.901763","status":"completed"},"tags":[]},"outputs":[],"source":["## same way tfifd , glove can be used "]},{"cell_type":"code","execution_count":33,"id":"c815bc57","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:36.946444Z","iopub.status.busy":"2023-12-09T15:12:36.946207Z","iopub.status.idle":"2023-12-09T15:12:36.949707Z","shell.execute_reply":"2023-12-09T15:12:36.948858Z"},"papermill":{"duration":0.018763,"end_time":"2023-12-09T15:12:36.951751","exception":false,"start_time":"2023-12-09T15:12:36.932988","status":"completed"},"tags":[]},"outputs":[],"source":["## use token"]},{"cell_type":"code","execution_count":34,"id":"a17fd3e6","metadata":{"execution":{"iopub.execute_input":"2023-12-09T15:12:36.979301Z","iopub.status.busy":"2023-12-09T15:12:36.97877Z","iopub.status.idle":"2023-12-09T15:12:36.982109Z","shell.execute_reply":"2023-12-09T15:12:36.98151Z"},"papermill":{"duration":0.018806,"end_time":"2023-12-09T15:12:36.983621","exception":false,"start_time":"2023-12-09T15:12:36.964815","status":"completed"},"tags":[]},"outputs":[],"source":["## SIMPLE RNN \n","## DATASET CLASS\n","## load data into dataloader\n","## define architecture\n","## dfine loss function\n","## define optimizer \n","## train\n","## evaluate\n","## predict"]},{"cell_type":"code","execution_count":null,"id":"99046f7f","metadata":{"papermill":{"duration":0.013259,"end_time":"2023-12-09T15:12:37.010174","exception":false,"start_time":"2023-12-09T15:12:36.996915","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"737175e7","metadata":{"papermill":{"duration":0.012556,"end_time":"2023-12-09T15:12:37.035703","exception":false,"start_time":"2023-12-09T15:12:37.023147","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"be64d20a","metadata":{"papermill":{"duration":0.012203,"end_time":"2023-12-09T15:12:37.060401","exception":false,"start_time":"2023-12-09T15:12:37.048198","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"b0319ac3","metadata":{"papermill":{"duration":0.012145,"end_time":"2023-12-09T15:12:37.084997","exception":false,"start_time":"2023-12-09T15:12:37.072852","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"3350f1a6","metadata":{"papermill":{"duration":0.01207,"end_time":"2023-12-09T15:12:37.109408","exception":false,"start_time":"2023-12-09T15:12:37.097338","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":2703900,"sourceId":19018,"sourceType":"competition"}],"dockerImageVersionId":30587,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":21.246599,"end_time":"2023-12-09T15:12:38.04083","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-12-09T15:12:16.794231","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}